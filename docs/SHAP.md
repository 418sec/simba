# Model explainability and calculating feature SHAP values in SimBA

An understanding, and ability to explain and communicate, how and why machine learning models reach their different decisions, is important not only for the scientific method, but will help you compare the similarities and differences of disperate classifiers that has been generated by different annotators and institutions. Machine learning explainability metrics could help you answer questions like:

* Why does my classifier think some specific frames are attack events, while other frames are not attack events?
 
* Why does my tracking model think this is the nose of the animal?
 
* Why does the classifier generated by annotator X classify events so differently from the classifier generated by person Y? 

* Which, of several classifiers, classify events the way I would classify the the same events?
 
Explainability metrics are extremely important, as it is possible that the classifiers you are using can *appear* to look for the same behavioral target behaviours and features as the human observer (have strong *face validity*) while the classifier in fact looks as something very different from the human observer (weak *construct validity*). 

In this tutorial we will look at how we can use [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) within SimBA to calculate how much each feature contributes to the final behavioral classification score for each frame. Through this method we will get an verbalizable explanation for the classification score for each frame, such as: 

*Frame N on Video X was was classified as containing attack behavior mainly because of the small distance between animal A and B and that the movements of animal A was large. The movement of the animals increased the atatck certainty with 30%, and the distance between them increased the atatck certainty with a further 80%*. 
 
 
In brief, when using SHAP, each feature is evaluated independently, and the final classification probability is distributed among the individual features according to their contribution, as evaluated after permutations within the order of feature-introductions into the classification scenario:

![](/simba/blob/master/images/SHAP1.png "SHAP1")

<p align="center">
  <src="https://github.com/sgoldenlab/simba/blob/master/images/SHAP1.png">
</p>




To read more about SHAP values, see the [SHAP GitHub repository](https://github.com/slundberg/shap) or the [SHAP paper in nature machine learning intelligence](https://www.nature.com/articles/s42256-019-0138-9).  
