# Model explainability and calculating feature SHAP values in SimBA

<p align="center">
<img src="https://github.com/sgoldenlab/simba/blob/master/images/SHAP0.png" />
</p>



An understanding, and ability to explain and communicate, how and why machine learning models reach their different decisions, is important not only for the scientific method, but will help you compare the similarities and differences of disperate classifiers that has been generated by different annotators and institutions. Machine learning explainability metrics could help you answer questions like:

* Why does my classifier think some specific frames are attack events, while it think other frames are non-attack events?
 
* Why does my tracking model think this is the nose of the animal?
 
* Why does the classifier generated by annotator X classify events so differently from the classifier generated by person Y? 

* Which, of several classifiers, classify events the way I would classify the the same events?
 
Explainability metrics are extremely important, as it is possible that the classifiers you are using can *appear* to look for the same behavioral target behaviours and features as the human observer (have strong *face validity*) while the classifier in fact looks as something very different from the human observer (weak *construct validity*). 

In this tutorial we will look at how we can use [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) within SimBA to calculate how much each feature contributes to the final behavioral classification score for each frame. Through this method we will get an verbalizable explanation for the classification score for each frame, such as: 

*Frame N on Video X was was classified as containing attack behavior mainly because of the small distance between animal A and B and that the movements of animal A was large. The movement of the animals increased the atatck certainty with 30%, and the distance between them increased the atatck certainty with a further 80%*. 
 
 
In brief, when using SHAP, each feature is evaluated independently, and the final classification probability is distributed among the individual features according to their contribution, as evaluated after permutations within the order of feature-introductions into the classification scenario:

<p align="center">
<img src="https://github.com/sgoldenlab/simba/blob/master/images/SHAP1.png" />
</p>

Where the *base probability* in the figure above, is the probability if picking a frame that contains your behavior by chance (e.g., if half of your video frames contain attack events, then the base probability will be 50%). To read more about SHAP values, see the [SHAP GitHub repository](https://github.com/slundberg/shap) which SimBA wraps, or read the excellent [SHAP paper in nature machine learning intelligence](https://www.nature.com/articles/s42256-019-0138-9). 

## Part 1: Generate a dataset

SimBA calculates SHAP values for the classifier at the same time as the model is being trained. Thus, before analysing SHAP scores, we need a dataset that contains behavioral annotatiopns. Therefore, you will need complete the steps detailed in the [Scenario 1 tutorial](https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md) **Part 1 Step 1** to **Part 2 Step 6**. That is, you will need to complete everything from [Creating a project](https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#part-1-create-a-new-project-1) up to, and including [labelling behavioral events](https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-6-label-behavior-ie-create-annotations-for-predictive-classifiers). 

>Note: If you already have annotations generated elsewhere (e.g., downloaded from the [SimBA OSF repository](https://osf.io/d69jt/), you may not have to go through **Part 1 Step 1** to **Part 2 Step 6** as detailed above. When calculating the SHAP values, SiMBA will loook inside your `project_folder/csv/targets_inserted` subdirectory for annotated files (just as SimBA does when generating the classifier). The important thing is that this folder is populated with files containing behavioral annotations. 

## Part 2: Compute SHAP scores

Next, navigate to the `Train machine model` tab and click on `Settings`. In the pop-up window, fill out your model `hyperparameter` settings as described [HERE]
(https://github.com/sgoldenlab/simba/blob/master/docs/Scenario1.md#step-7-train-machine-model. At the bottom of the `Settings` pop up window, you will see these entry boxes. Tick the `Calculate SHAP values` entry box to enable them:

<p align="center">
<img src="https://github.com/sgoldenlab/simba/blob/master/images/SHAP2.png" />
</p>







