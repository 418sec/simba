# Model explainability and calculating feature SHAP values in SimBA

An understanding, and ability to explain and communicate, how and why machine learning models reach their different decisions, is important not only for the scientific method, but will help you compare the similarities and differences of disperate classifiers that has been generated by different annotators and institutions. Machine learning explainability metrics can help you answer questions like:

 (i) Why does my classifier think these specific frames are attack events?
 (ii) Why does my model think this is the nose of the animal?
 (iii) Why does the classifier generated by annotator X classify events so differently from the classifier generated by person Y? 
 
Explainability metrics are extremely important, as it is possible that the classifiers you are using can *appear* to look for the same behavioral target behaviours and features as the human observer (have strong *face validity*) while the classifier in fact looks as something very different from the human observer (weak *construct validity*). 

In this tutorial we will look at how we can use [SHAP (SHapley Additive exPlanations)](https://github.com/slundberg/shap) within SimBA to calculate how much each feature contributes to the final behavioral classification score for each frame. Through this method we will get an verbalizable explanation for the classification score for each frame, such as: 

*Frame N on Video X was was classified as containing attack behavior mainly because of the small distance between animal A and B and that the movements of animal A was large. The movement of the animals increased the atatck certainty with 30%, and the distance between them increased the atatck certainty with a further 80%*. 
 
